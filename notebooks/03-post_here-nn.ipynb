{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: Subreddit Predictor\n",
    "\n",
    "## Recommendation API - 1.3\n",
    "\n",
    "> Using Keras and Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Model #3 - Neural Networks and Pre-Trained Embeddings\n",
    "\n",
    "The third iteration of the model for predicting the most appropriate subreddits for a given post will use Keras to architect a neural network to make use of a pre-trained word embedding.\n",
    "\n",
    "- [reddit self-post classification task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load _le_ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === sklearn imports === #\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Keras imports === #\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load the dataset === #\n",
    "# Tab-separated, 10k rows\n",
    "df1 = pd.read_csv(\"dataset.csv\", sep=\"\\t\")\n",
    "\n",
    "# Confirm it is correct shape\n",
    "assert df1.shape[0] == 10000\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# === Arrange data into feature and target === #\n",
    "\n",
    "# MVP model only uses 'selftext' feature\n",
    "X = df1[\"selftext\"]\n",
    "\n",
    "# Predict the subreddit of each post\n",
    "y = df1[\"subreddit\"]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([920, 931, 161, 827, 669, 822,  39, 650])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each class of the target into a number\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() # Instantiate a new encoder instance\n",
    "y = le.fit_transform(y)  # Fit and transform label data\n",
    "\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56166 unique tokens.\n",
      "Shape of data tensor: (10000, 100)\n",
      "Shape of label tensor: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# === Tokenize using keras === #\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import numpy as np\n",
    "\n",
    "# Parameters\n",
    "maxlen = 100\n",
    "max_words = 20000  # Use only top 20k words\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Build word index and pad sequences\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Found {len(word_index)} unique tokens.\")\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(y)\n",
    "print(f\"Shape of data tensor: {data.shape}\")\n",
    "print(f\"Shape of label tensor: {labels.shape}\")\n",
    "\n",
    "# Set up train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1013)              130677    \n",
      "=================================================================\n",
      "Total params: 2,822,261\n",
      "Trainable params: 2,822,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# === LSTM Recurrent Neural Network using Keras === #\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "# Construct the network, layer by layer\n",
    "model.add(Embedding(max_words, 128))  # Word embedding layer\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
    "\n",
    "# Compile the network\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "8000/8000 [==============================] - 73s 9ms/sample - loss: 6.9209 - accuracy: 0.0011 - val_loss: 6.9209 - val_accuracy: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150ac9a50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Train and test the network === #\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === Neural Network using Keras Layers === #\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "# Construct the network, layer by layer\n",
    "model.add(Embedding(max_words, 128, input_length=maxlen))  # Word embedding layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the network\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1013)\n"
     ]
    }
   ],
   "source": [
    "# === Create predictions on test feature === #\n",
    "y_pred_proba = nb.predict_proba(X_test_sparse)\n",
    "\n",
    "print(y_pred_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([279, 279, 279, 279, 598, 560, 185, 553, 185, 185])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === For each prediction, find the index with the highest probability === #\n",
    "# import numpy as np\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.108\n",
      "precision@3 = 0.1785\n",
      "precision@5 = 0.215\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate performance using precision-at-k === #\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))\n",
    "\n",
    "# 1,000,000 records\n",
    "# precision@1 = 0.6732724580454097\n",
    "# precision@3 = 0.8058588351431392\n",
    "# precision@5 = 0.8481391905231984\n",
    "\n",
    "# 100,000 records\n",
    "# precision@1 = 0.45935\n",
    "# precision@3 = 0.6075\n",
    "# precision@5 = 0.665\n",
    "\n",
    "# 10,000 records\n",
    "# precision@1 = 0.108\n",
    "# precision@3 = 0.1785\n",
    "# precision@5 = 0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Generate Predictions from new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example post === #\n",
    "\n",
    "# The example comes from 'r/learnprogramming'\n",
    "post = \"\"\"I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and \"leet code\" skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days?\n",
    "\n",
    "Edit:\n",
    "\n",
    "A lot of people are saying \"the company is a sinking ship don't even go to the interview\". I just want to add that the position was always for a \"junior backend engineer\". This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I'm interviewing for. I'm sure they're mainly interested in seeing my understanding of good backend principles and software design, it's not a senior lead Java position.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to serve predictions === #\n",
    "# The main functionality of the predict API endpoint\n",
    "\n",
    "def predict(post: str, n: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Serve subreddit predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    post : string\n",
    "        Selftext that needs a home.\n",
    "    n    : integer\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary formatted as follows:\n",
    "        [{'subreddit': 'PLC', 'proba': 0.014454},\n",
    "         ...\n",
    "         {'subreddit': 'Rowing', 'proba': 0.005206}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorize the post -> sparse doc-term matrix\n",
    "    post_vec = vocab.transform([post])\n",
    "    \n",
    "    # Generate predicted probabilities from trained model\n",
    "    proba = nb.predict_proba(post_vec)\n",
    "    \n",
    "    # Wrangle into correct format\n",
    "    return (pd\n",
    "                .DataFrame(proba, columns=[le.classes_])  # Classes as column names\n",
    "                .T  # Transpose so column names become index\n",
    "                .reset_index()  # Pull out index into a column\n",
    "                .rename(columns={\"level_0\": \"subreddit\", 0: \"proba\"})  # Rename for aesthetics\n",
    "                .sort_values(by=\"proba\", ascending=False)  # Sort by probability\n",
    "                .iloc[:n]  # n-top predictions to serve\n",
    "                .to_dict(orient=\"records\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'PLC', 'proba': 0.014454003455867464},\n",
       " {'subreddit': 'sales', 'proba': 0.008377114822879663},\n",
       " {'subreddit': 'AskHR', 'proba': 0.007373675677628392},\n",
       " {'subreddit': 'OccupationalTherapy', 'proba': 0.00561548847793111},\n",
       " {'subreddit': 'Rowing', 'proba': 0.005206212277479405}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test out the function === #\n",
    "post_pred = predict(post)  # Default is 5 results\n",
    "post_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test it out with another dummy post === #\n",
    "\n",
    "# This one comes from r/suggestmeabook\n",
    "post2 = \"\"\"I've been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I've read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don't like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn't have to be my type of novel. I'm open to experience after all. I need your help guys. Thanks in advance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'PLC', 'proba': 0.008913827946281429},\n",
       " {'subreddit': 'JUSTNOMIL', 'proba': 0.005364412871980992},\n",
       " {'subreddit': 'vaginismus', 'proba': 0.005343515334435943},\n",
       " {'subreddit': 'Rowing', 'proba': 0.004869320257487137},\n",
       " {'subreddit': 'hookah', 'proba': 0.00434668954125044},\n",
       " {'subreddit': 'dresdenfiles', 'proba': 0.004295750109887567},\n",
       " {'subreddit': 'StudentLoans', 'proba': 0.004077573292318194},\n",
       " {'subreddit': 'productivity', 'proba': 0.004050498345338448},\n",
       " {'subreddit': 'flexibility', 'proba': 0.003925624264099203},\n",
       " {'subreddit': 'dpdr', 'proba': 0.003923252551916036}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === This time with 10 results === #\n",
    "post2_pred = predict(post2, n=10)\n",
    "post2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Picklization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create pickle func to make pickling (a little) easier === #\n",
    "\n",
    "def picklizer(to_pickle, filename, path):\n",
    "    \"\"\"\n",
    "    Creates a pickle file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    to_pickle : Python object\n",
    "        The trained / fitted instance of the \n",
    "        transformer or model to be pickled.\n",
    "    filename : string\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "    path : string or path-like object\n",
    "        The path to the desired output directory.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Create the path to save location\n",
    "    picklepath = os.path.join(path, filename)\n",
    "\n",
    "    # Use context manager to open file\n",
    "    with open(picklepath, \"wb\") as p:\n",
    "        pickle.dump(to_pickle, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Picklize! === #\n",
    "filepath = \"../assets\"\n",
    "\n",
    "# Export pipeline as pickle\n",
    "picklizer(vocab, \"03_pipe.pkl\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
