{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: Subreddit Predictor\n",
    "\n",
    "## Recommendation API - 1.4\n",
    "\n",
    "> aka: the MVP ClassifierÂ® Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Intro - MVP Classifier (Model #4)\n",
    "\n",
    "The fourth (at least) iteration of the model for recommending (predicting) appropriate subreddit(s).\n",
    "\n",
    "The model will be trained using the [reddit self-post classification task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), available on Kaggle thanks to [Evolution AI](https://evolution.ai//blog/page/5/an-imagenet-like-text-classification-task-based-on-reddit-posts/).\n",
    "\n",
    "The full dataset includes 1,013,000 rows (1000 records each from 1013 subreddits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === sklearn imports === #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             subreddit  \\\n",
       "0   0  talesfromtechsupport   \n",
       "1   1               teenmom   \n",
       "2   2                Harley   \n",
       "3   3          ringdoorbell   \n",
       "4   4                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load the saved version === #\n",
    "df1 = pd.read_csv(\"rspct_100k.csv\", sep=\"\\t\")\n",
    "\n",
    "# === First looks === #\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 4), (20000, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Split up dataset into train and test === #\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train, 20% test, stratified on the target\n",
    "train, test = train_test_split(df1, test_size=0.2)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000,) (20000,)\n",
      "(80000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "# === Arrange data into feature and target === #\n",
    "\n",
    "# MVP model only uses 'selftext' feature\n",
    "X_train = train[\"selftext\"]\n",
    "X_test = test[\"selftext\"]\n",
    "\n",
    "# Predict the subreddit of each post\n",
    "y_train = train[\"subreddit\"]\n",
    "y_test = test[\"subreddit\"]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([565, 179, 699,  76, 363, 824, 284, 828])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each class of the target into a number\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() # Instantiate a new encoder instance\n",
    "le.fit(y_train)  # Fit it on training label data\n",
    "\n",
    "# Transform both using the train-fit instance\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "y_train[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e"
   },
   "outputs": [],
   "source": [
    "# === Vectorize! === #\n",
    "\n",
    "# Extract features from the text data using bag-of-words (single words + bigrams).\n",
    "# Uses tfidf weighting (helps a little for Naive Bayes in general).\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100000,\n",
    "    min_df=10,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=None,  # TODO: try out spacy's or sklearn's stopwords\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the feature column to create vocab (doc-term matrix)\n",
    "vocab = tfidf.fit(X_train)\n",
    "\n",
    "# Get sparse document-term matrices\n",
    "X_train_sparse = vocab.transform(X_train)\n",
    "X_test_sparse = vocab.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 3000), (20000, 3000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Feature Selection === #\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "selector = SelectKBest(chi2, 3000)\n",
    "\n",
    "selector.fit(X_train_sparse, y_train)\n",
    "\n",
    "X_train_select = selector.transform(X_train_sparse)\n",
    "X_test_select  = selector.transform(X_test_sparse)\n",
    "\n",
    "X_train_select.shape, X_test_select.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=32, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Baseline RandomForest model === #\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate and train the model\n",
    "rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122, 925, 842, 330, 402, 870,  48, 842, 402, 842])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Create predictions on test feature === #\n",
    "y_pred_proba_rfc = rfc.predict_proba(X_test)\n",
    "\n",
    "# === For each prediction, find the index with the highest probability === #\n",
    "y_pred_rfc = np.argmax(y_pred_proba_rfc, axis=1)\n",
    "y_pred_rfc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.27855\n",
      "precision@3 = 0.319\n",
      "precision@5 = 0.32755\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate performance using precision-at-k === #\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred_rfc))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba_rfc, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba_rfc, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a pretty solid Random Forest, the results are not great.\n",
    "\n",
    "Let's see what the predictions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === Function to serve predictions - Random Forest === #\n",
    "\n",
    "\n",
    "def predict(post: str, n: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Serve subreddit predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    post : string\n",
    "        Selftext that needs a home.\n",
    "    n    : integer\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary formatted as follows:\n",
    "        [{'subreddit': 'PLC', 'proba': 0.014454},\n",
    "         ...\n",
    "         {'subreddit': 'Rowing', 'proba': 0.005206}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorize the post -> sparse doc-term matrix\n",
    "    post_vec = vocab.transform([post])\n",
    "    \n",
    "    # Generate predicted probabilities from trained model\n",
    "    proba = rfc.predict_proba(post_vec)\n",
    "    \n",
    "    # Wrangle into correct format\n",
    "    return (pd\n",
    "                .DataFrame(proba, columns=[le.classes_])  # Classes as column names\n",
    "                .T  # Transpose so column names become index\n",
    "                .reset_index()  # Pull out index into a column\n",
    "                .rename(columns={\"level_0\": \"subreddit\", 0: \"proba\"})  # Rename for aesthetics\n",
    "                .sort_values(by=\"proba\", ascending=False)  # Sort by probability\n",
    "                .iloc[:n]  # n-top predictions to serve\n",
    "                .to_dict(orient=\"records\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.4, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Naive Bayes model === #\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate and train the model\n",
    "nb = MultinomialNB(alpha=0.4)\n",
    "nb.fit(X_train_select, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104,  95, 240, 198, 714, 872, 177, 104, 507, 343])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Create predictions on test feature === #\n",
    "y_pred_proba = nb.predict_proba(X_test_select)\n",
    "\n",
    "# === For each prediction, find the index with the highest probability === #\n",
    "# import numpy as np\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.35865\n",
      "precision@3 = 0.44935\n",
      "precision@5 = 0.47965\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Predict subreddit from new input\n",
    "\n",
    "Now that our model is trained and we have our baseline, we can use it to predict what subreddit would belong to a new piece of data (a post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === Function to serve predictions - Naive Bayes === #\n",
    "\n",
    "\n",
    "def predict(post: str, n: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Serve subreddit predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    post : string\n",
    "        Selftext that needs a home.\n",
    "    n    : integer\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary formatted as follows:\n",
    "        [{'subreddit': 'PLC', 'proba': 0.014454},\n",
    "         ...\n",
    "         {'subreddit': 'Rowing', 'proba': 0.005206}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorize the post -> sparse doc-term matrix\n",
    "    post_vec = vocab.transform([post])\n",
    "    \n",
    "    # Generate predicted probabilities from trained model\n",
    "    proba = nb.predict_proba(post_vec)\n",
    "    \n",
    "    # Wrangle into correct format\n",
    "    return (pd\n",
    "                .DataFrame(proba, columns=[le.classes_])  # Classes as column names\n",
    "                .T  # Transpose so column names become index\n",
    "                .reset_index()  # Pull out index into a column\n",
    "                .rename(columns={\"level_0\": \"subreddit\", 0: \"proba\"})  # Rename for aesthetics\n",
    "                .sort_values(by=\"proba\", ascending=False)  # Sort by probability\n",
    "                .iloc[:n]  # n-top predictions to serve\n",
    "                .to_dict(orient=\"records\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to serve predictions === #\n",
    "# The main functionality of the predict API endpoint\n",
    "\n",
    "def predict(post: str, n: int = 5):\n",
    "    \"\"\"\n",
    "    Serve subreddit predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    post : string\n",
    "        Selftext that needs a home.\n",
    "    n    : integer\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary formatted as follows:\n",
    "        [{'subreddit': 'PLC', 'proba': 0.014454},\n",
    "         ...\n",
    "         {'subreddit': 'Rowing', 'proba': 0.005206}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorize the post -> sparse doc-term matrix\n",
    "    post_sparse = vocab.transform([post])\n",
    "    \n",
    "    # Feature selection\n",
    "    post_select = selector.transform(post_sparse)\n",
    "    \n",
    "    # Generate predicted probabilities from trained model\n",
    "    proba = nb.predict_proba(post_select)\n",
    "    \n",
    "    # Wrangle into correct format\n",
    "    return (pd\n",
    "                .DataFrame(proba, columns=[le.classes_])  # Classes as column names\n",
    "                .T  # Transpose so column names become index\n",
    "                .reset_index()  # Pull out index into a column\n",
    "                .rename(columns={\"level_0\": \"subreddit\", 0: \"proba\"})  # Rename for aesthetics\n",
    "                .sort_values(by=\"proba\", ascending=False)  # Sort by probability\n",
    "                .iloc[:n]  # n-top predictions to serve\n",
    "                .to_dict(orient=\"records\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'HotPeppers', 'proba': 0.0015314459001810564},\n",
       " {'subreddit': 'DragonsDogma', 'proba': 0.0013730010156944878},\n",
       " {'subreddit': 'cosplay', 'proba': 0.0013357695514130226},\n",
       " {'subreddit': 'radiohead', 'proba': 0.0013233044911954515},\n",
       " {'subreddit': 'Charity', 'proba': 0.0012862958864824758}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_science = \"\"\"Is there an evolutionary benefit to eating spicy food that lead to consumption across numerous cultures throughout history? Or do humans just like the sensation?\n",
    "\n",
    "I love spicy food and have done ever since I tried it. By spicy I mean HOT, like chilli peppers (we say spicy in England, I don't mean to state the obvious I'm just not sure if that's a global term and I've assumed too much before). I love a vast array of spicy foods from all around the world. I was just wondering if there was some evolutionary basis as to why spicy food managed to become some widely consumed historically. Though there seem to\n",
    "\n",
    "It way well be that we just like a tingly mouth, the simple things in life.\"\"\"\n",
    "\n",
    "science_recs = predict(post_science)\n",
    "science_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'DragonsDogma', 'proba': 0.0013750000000000001},\n",
       " {'subreddit': 'cosplay', 'proba': 0.001337499999999999},\n",
       " {'subreddit': 'radiohead', 'proba': 0.0013249999999999994},\n",
       " {'subreddit': 'Charity', 'proba': 0.0012874999999999998},\n",
       " {'subreddit': 'titanfall', 'proba': 0.0012749999999999992},\n",
       " {'subreddit': 'Chromecast', 'proba': 0.0012749999999999992},\n",
       " {'subreddit': 'snapchat', 'proba': 0.0012500000000000002},\n",
       " {'subreddit': '3Dprinting', 'proba': 0.0012500000000000002},\n",
       " {'subreddit': 'resumes', 'proba': 0.0012500000000000002},\n",
       " {'subreddit': 'sales', 'proba': 0.0012374999999999995}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test post from r/buildmeapc === #\n",
    "\n",
    "post_pc = \"\"\"I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldnât start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldnât be more excited!\"\"\"\n",
    "\n",
    "post_pc_recs = predict(post_pc, 10)\n",
    "post_pc_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'javahelp', 'proba': 0.0023090280144774347},\n",
       " {'subreddit': 'androiddev', 'proba': 0.0017914570853405122},\n",
       " {'subreddit': 'cscareerquestions', 'proba': 0.001611167703091607},\n",
       " {'subreddit': 'interviews', 'proba': 0.0014772532994282328},\n",
       " {'subreddit': 'DragonsDogma', 'proba': 0.0013503849085304746}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Example post from 'r/learnprogramming' === #\n",
    "\n",
    "post = \"\"\"I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and \"leet code\" skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days?\n",
    "\n",
    "Edit:\n",
    "\n",
    "A lot of people are saying \"the company is a sinking ship don't even go to the interview\". I just want to add that the position was always for a \"junior backend engineer\". This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I'm interviewing for. I'm sure they're mainly interested in seeing my understanding of good backend principles and software design, it's not a senior lead Java position.\"\"\"\n",
    "\n",
    "# === Test out the function === #\n",
    "post_pred = predict(post)  # Default is 5 results\n",
    "post_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'DragonsDogma', 'proba': 0.0013712105334414554},\n",
       " {'subreddit': 'radiohead', 'proba': 0.0013476440520005367},\n",
       " {'subreddit': 'cosplay', 'proba': 0.0013340494974593818},\n",
       " {'subreddit': 'MaladaptiveDreaming', 'proba': 0.0012971874649116608},\n",
       " {'subreddit': 'Charity', 'proba': 0.0012846867326412827},\n",
       " {'subreddit': 'titanfall', 'proba': 0.001272269133622847},\n",
       " {'subreddit': 'Chromecast', 'proba': 0.0012709935975766793},\n",
       " {'subreddit': 'snapchat', 'proba': 0.0012473813271700195},\n",
       " {'subreddit': 'resumes', 'proba': 0.00124646426422597},\n",
       " {'subreddit': '3Dprinting', 'proba': 0.0012457958646503995}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test it out with another dummy post === #\n",
    "\n",
    "# This one comes from r/suggestmeabook\n",
    "post2 = \"\"\"I've been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I've read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don't like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn't have to be my type of novel. I'm open to experience after all. I need your help guys. Thanks in advance.\"\"\"\n",
    "\n",
    "# === This time with 10 results === #\n",
    "post2_pred = predict(post2, n=10)\n",
    "post2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pickle Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create pickle func to make pickling (a little) easier === #\n",
    "\n",
    "def picklizer(to_pickle, filename, path):\n",
    "    \"\"\"\n",
    "    Creates a pickle file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    to_pickle : Python object\n",
    "        The trained / fitted instance of the \n",
    "        transformer or model to be pickled.\n",
    "    filename : string\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "    path : string or path-like object\n",
    "        The path to the desired output directory.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Create the path to save location\n",
    "    picklepath = os.path.join(path, filename)\n",
    "\n",
    "    # Use context manager to open file\n",
    "    with open(picklepath, \"wb\") as p:\n",
    "        pickle.dump(to_pickle, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Picklize! === #\n",
    "filepath = \"./pickles\"  # Change this accordingly\n",
    "\n",
    "# Export LabelEncoder as pickle\n",
    "picklizer(le, \"05_le.pkl\", filepath)\n",
    "\n",
    "# Export selector as pickle\n",
    "picklizer(selector, \"05_selector.pkl\", filepath)\n",
    "\n",
    "# Export vectorizer as pickle\n",
    "picklizer(vocab, \"05_vocab.pkl\", filepath)\n",
    "\n",
    "# Export naive bayes model as pickle\n",
    "picklizer(nb, \"05_nb.pkl\", filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and consume pickles...\n",
    "\n",
    "> _Enjoy responsibly._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load the trained vectorizer === #\n",
    "import pickle\n",
    "\n",
    "le_path = os.path.join(filepath, \"04_le.pkl\")\n",
    "\n",
    "# Use context manager to open and load pickle\n",
    "with open(le_path, \"rb\") as p:\n",
    "    le = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load the trained vectorizer === #\n",
    "import pickle\n",
    "\n",
    "vocab_path = os.path.join(filepath, \"04_vocab.pkl\")\n",
    "\n",
    "# Use context manager to open and load pickle\n",
    "with open(vocab_path, \"rb\") as p:\n",
    "    vocab = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load the somewhat-trained Random Forest classifier === #\n",
    "# import pickle\n",
    "\n",
    "rfc_path = os.path.join(filepath, \"04_rfc.pkl\")\n",
    "\n",
    "# Use context manager to open and load pickle\n",
    "with open(rfc_path, \"rb\") as p:\n",
    "    rfc = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subreddit': 'parrots', 'proba': 0.00211370360467277},\n",
       " {'subreddit': 'flexibility', 'proba': 0.002096921513234382},\n",
       " {'subreddit': 'bladeandsoul', 'proba': 0.002071709687128192},\n",
       " {'subreddit': 'Rowing', 'proba': 0.0020525020657867544},\n",
       " {'subreddit': 'StudentLoans', 'proba': 0.0020488032050522157},\n",
       " {'subreddit': 'gigantic', 'proba': 0.0019414817435439607},\n",
       " {'subreddit': 'PLC', 'proba': 0.0018997017900174518},\n",
       " {'subreddit': 'TransDIY', 'proba': 0.001892048211568393},\n",
       " {'subreddit': 'breastfeeding', 'proba': 0.0018844645436299533},\n",
       " {'subreddit': 'RocketLeague', 'proba': 0.0018758224984953434},\n",
       " {'subreddit': 'benzodiazepines', 'proba': 0.0018668138495396874},\n",
       " {'subreddit': 'The100', 'proba': 0.0018580496088653011},\n",
       " {'subreddit': 'minecraftsuggestions', 'proba': 0.0018453927006669075},\n",
       " {'subreddit': 'techsupport', 'proba': 0.0018442340100311113},\n",
       " {'subreddit': 'Blink182', 'proba': 0.0018239715732455283},\n",
       " {'subreddit': 'dpdr', 'proba': 0.0018163028805450827},\n",
       " {'subreddit': 'logorequests', 'proba': 0.001814948788130875},\n",
       " {'subreddit': 'notebooks', 'proba': 0.001803676671939633},\n",
       " {'subreddit': 'hacking', 'proba': 0.0017961381089489498},\n",
       " {'subreddit': 'homestead', 'proba': 0.0017950154127724502}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test out the pickled versions === #\n",
    "\n",
    "# This one comes from r/buildmeapc\n",
    "post3 = \"\"\"I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldnât start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldnât be more excited!\"\"\"\n",
    "\n",
    "# This time I'll pass in 20, because why not?\n",
    "post3_recs = predict(post3, 20)\n",
    "post3_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
