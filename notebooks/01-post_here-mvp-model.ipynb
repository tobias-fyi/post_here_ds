{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: Subreddit Predictor\n",
    "\n",
    "## Recommendation API - 1.0\n",
    "\n",
    "> aka: the MVP models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Intro - MVP\n",
    "\n",
    "The first iteration of the model for recommending (predicting) appropriate subreddit(s) will be built using a somewhat naive approach to text.\n",
    "\n",
    "The model will be trained using the [reddit self-post classification task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), available on Kaggle thanks to [Evolution AI](https://evolution.ai//blog/page/5/an-imagenet-like-text-classification-task-based-on-reddit-posts/).\n",
    "\n",
    "The dataset should include 1,013,000 rows (1000 records each from 1013 subreddits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import janitor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === sklearn imports === #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load the dataset === #\n",
    "data_filepath = \"/Users/Tobias/workshop/buildbox/post_here_ds/dox/reddit-selfposts/rspct.tsv\"\n",
    "df1 = pd.read_csv(data_filepath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1013000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === First looks === #\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MVP model will be trained on the selftext column only. Therefore, the only columns that are needed are `selftext` (feature, X) and `subreddit` (target, y).\n",
    "\n",
    "First, let's take a sample of the dataset to reduce the memory load and runtime of the training. The full dataset can be used after the MVP is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract sample of 1/10th of total records === #\n",
    "# Should be around 50k records\n",
    "df1 = df1.sample(frac=0.1, replace=True, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101300, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Confirm it worked as expected === #\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81040, 4), (20260, 4))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Split up dataset into train and test === #\n",
    "\n",
    "# 80% train, 20% test, stratified on the target\n",
    "train, test = train_test_split(df1, test_size=0.2, stratify=df1[\"subreddit\"])\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Arrange data into feature and target === #\n",
    "\n",
    "X_train = train[\"selftext\"]\n",
    "X_test = test[\"selftext\"]\n",
    "\n",
    "y_train = train[\"subreddit\"]\n",
    "y_test = test[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929865    I cant figure out if i like this card or not. ...\n",
       "9279      Kanyes apparently got 7 songs on this next alb...\n",
       "896004    I have done 2 tri's to date, 1 sprint and 1 in...\n",
       "952229    So, my parents know I'm genderqueer, though th...\n",
       "741843    [Mine Oddity](https://www.youtube.com/watch?v=...\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929865         FoWtcg\n",
       "9279            Kanye\n",
       "896004      triathlon\n",
       "952229    genderqueer\n",
       "741843    MemeEconomy\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trumpet            106\n",
       "weddingplanning    105\n",
       "aznidentity        104\n",
       "howyoudoin         103\n",
       "Blink182           103\n",
       "                  ... \n",
       "Volvo               59\n",
       "Grimdawn            59\n",
       "nihilism            59\n",
       "bash                58\n",
       "Lisk                54\n",
       "Name: subreddit, Length: 1013, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Look at number of classes === #\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "The vectorizer that will be used to convert the words into numbers will not analyze the text for meaning or anything like that - remember, this is the MVP model. We can get as crazy as we want after we have a working baseline.\n",
    "\n",
    "TF-IDF vectorization finds the unique aspects of documents of text, based on a simple count of the words within each document. In this context, \"document\" refers to an individual reddit post.\n",
    "\n",
    "The vectorizer can be instantiated then \"trained\" on the dataset. One way to think about the training step is that it builds a vectorized vocabulary of the words in the dataset.\n",
    "\n",
    "The TF-IDF implementation that will be used in the MVP comes from scikit-learn:\n",
    "\n",
    "> [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "A custom tokenizer function can be passed into the vectoirizer to increase the quality of the tokens (number representations of words). The tokenizer function will use the NLP library [spaCy](https://spacy.io/usage/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Spacy === #\n",
    "import spacy\n",
    "\n",
    "# Load the medium-sized english language model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([132, 192, 949, 617, 237, 327,  62, 105])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each category of the target into a number\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "y_train[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom tokenizer function === #\n",
    "\n",
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Extracts nouns and adjectives from a string of text.\n",
    "    Returns a list of spacy token.lemma objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    na_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (\n",
    "            ((token.is_stop == False) and (token.is_punct == False))\n",
    "            and (token.pos_ == \"NOUN\")\n",
    "            or (token.pos_ == \"ADJ\")\n",
    "        ):\n",
    "            na_tokens.append(token.lemma_.strip().lower())\n",
    "\n",
    "    return na_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Vectorize the data === #\n",
    "\n",
    "# Instantiate the vectorizer object\n",
    "# This extracts features from the text using a bag-of-words approach\n",
    "# To start, the default tokenizer function will be used (instead of the above)\n",
    "tfidf = TfidfVectorizer(\n",
    "#     tokenizer=tokenize,\n",
    "    stop_words=\"english\",\n",
    "    max_features=100000,\n",
    "    min_df=0.025,\n",
    "    max_df=0.98\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the corpus to create the vocabulary\n",
    "dtm = tfidf.fit(X_train)\n",
    "\n",
    "# Get sparse doc-term matrix / word counts\n",
    "# This is done by using the trained vocabulary to transform the corpus into vectors\n",
    "sparse = tfidf.transform(X_train)\n",
    "\n",
    "# Get the feature names (words) to use as column names\n",
    "# Also, convert the sparse matrix into dense form - fill in empty counts with 0\n",
    "vdtm = pd.DataFrame(sparse.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>advance</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wrong</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154543</td>\n",
       "      <td>0.295731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510678</td>\n",
       "      <td>0.191263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100   12   15   20   30  able  actually  add  advance  ...  work  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0   0.0       0.0  0.0      0.0  ...   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0   0.0       0.0  0.0      0.0  ...   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0   0.0       0.0  0.0      0.0  ...   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0   0.0       0.0  0.0      0.0  ...   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0   0.0       0.0  0.0      0.0  ...   0.0   \n",
       "\n",
       "   working  works  world  worth  wouldn  wrong       www      year     years  \n",
       "0      0.0    0.0    0.0    0.0     0.0    0.0  0.000000  0.000000  0.000000  \n",
       "1      0.0    0.0    0.0    0.0     0.0    0.0  0.000000  0.000000  0.000000  \n",
       "2      0.0    0.0    0.0    0.0     0.0    0.0  0.000000  0.154543  0.295731  \n",
       "3      0.0    0.0    0.0    0.0     0.0    0.0  0.000000  0.000000  0.000000  \n",
       "4      0.0    0.0    0.0    0.0     0.0    0.0  0.510678  0.191263  0.000000  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Preview the resulting dataframe of vectorized tokens === #\n",
    "pd.options.display.max_rows = 200\n",
    "vdtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "The goal of the API is to be able to recommend subreddits with posts that are the most similar to the input text. With that in mind, we can convert the input document into a vector that matches the format of the dataset (transformed via the vocabulary that was generated in the vectorization step).\n",
    "\n",
    "That input vector can be fed into a algorithm that finds similarities between the input and the dataset. In the case of this MVP, the algorithm is called nearest neighbors.\n",
    "\n",
    "As the name suggests, the algorithm returns the so-called \"nearest neighbors\" to the input vector, based on a distance metric. Ideally, these will be the subreddits with content that is most similar to the input.\n",
    "\n",
    "The nearest neighbor implementation that will be used in the MVP comes from scikit-learn: \n",
    "\n",
    "> [sklearn.neighbors.NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)\n",
    "\n",
    "Classification with KNearestNeighbor (Post-MVP)\n",
    "\n",
    "> [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict_proba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Train the nearest neighbor model === #\n",
    "\n",
    "# Instantiate the NN model\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Instantiate the model with initial parameters - 10 neighbors\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "\n",
    "# Fit the NN model on TF-IDF Vectors\n",
    "nn.fit(vdtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Find neighbors for example input post === #\n",
    "\n",
    "# The example comes from 'r/learnprogramming'\n",
    "post = \"\"\"I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and \"leet code\" skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days?\n",
    "\n",
    "Edit:\n",
    "\n",
    "A lot of people are saying \"the company is a sinking ship don't even go to the interview\". I just want to add that the position was always for a \"junior backend engineer\". This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I'm interviewing for. I'm sure they're mainly interested in seeing my understanding of good backend principles and software design, it's not a senior lead Java position.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Vectorize the example post using the trained vocab === #\n",
    "post_sparse = tfidf.transform([post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.03440271, 1.04268939, 1.06792867]]),\n",
       " array([[ 9019,  5143,  2281, 71968, 72305, 34391, 70197, 65457, 19212,\n",
       "         34797]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the transformed (vectorized) input through the NN model\n",
    "# See where the input fits into the corpus, and return the 10 nearest neighbors\n",
    "# In order to use the vectorized input, the labels can be reverse_transformed back through the encoder\n",
    "# Or looked up via the index\n",
    "rec_array = nn.kneighbors(post_sparse.todense(), n_neighbors=10)\n",
    "rec_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9019,  5143,  2281, 71968, 72305, 34391, 70197, 65457, 19212,\n",
       "       34797])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the second item in the outer array\n",
    "# This is the list of the review indices that are 'closest' to input\n",
    "rec_id_list = rec_array[1][0]\n",
    "rec_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482379      Cloververse\n",
       "819094    adventuretime\n",
       "328843          TheWire\n",
       "195943     bladeandsoul\n",
       "571435             SCCM\n",
       "106538    callofcthulhu\n",
       "657019           Ripple\n",
       "942412            Cisco\n",
       "289069      woodworking\n",
       "36363       freemasonry\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hydrate that list with the rest of the data from the (almost) original dataframe\n",
    "recommendations = df1.iloc[rec_id_list][\"subreddit\"]\n",
    "\n",
    "# The resulting dataframe should have 10 rows\n",
    "assert recommendations.shape[0] == 10\n",
    "\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 Result\n",
    "\n",
    "Out of the top ten nearest neighbors, only two of them have anything at all to do with programming.\n",
    "\n",
    "That's not very good. Not good at all.\n",
    "\n",
    "Will we be able to do better than that?!\n",
    "\n",
    "> Find out next time, on Post Here at `02-post_here-mvp-model.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === But first, let's try with a different example post === #\n",
    "\n",
    "# This one comes from r/suggestmeabook\n",
    "post2 = \"\"\"I've been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I've read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don't like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn't have to be my type of novel. I'm open to experience after all. I need your help guys. Thanks in advance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Predict function === #\n",
    "# To make the recommendation and integration with the Flask api easier,\n",
    "# Here's a function to generate the recommendations\n",
    "\n",
    "def recommend(req, n=10):\n",
    "    \"\"\"Function to recommend top n subreddits given a request.\"\"\"\n",
    "    # Create vector from request\n",
    "    req_vec = tfidf.transform([req])\n",
    "\n",
    "    # Get indexes for n nearest neighbors\n",
    "    top_id = nn.kneighbors(req_vec.todense(), n_neighbors=n)[1][0]\n",
    "\n",
    "    # Index-locate the neighbors in original dataframe\n",
    "    top_array = df1.iloc[top_id][\"subreddit\"]\n",
    "\n",
    "    return top_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270901    netneutrality\n",
       "195943     bladeandsoul\n",
       "819094    adventuretime\n",
       "571435             SCCM\n",
       "657019           Ripple\n",
       "328843          TheWire\n",
       "482379      Cloververse\n",
       "106538    callofcthulhu\n",
       "391942      learnpython\n",
       "860458             volt\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Run it again! === #\n",
    "post2_recs = recommend(post2)\n",
    "post2_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 Results, Part 2\n",
    "\n",
    "Again, the recommendations do not seem to be very good. However, in the interest of getting the MVP up and working. Let's pickle this model anyways, to start integrating it with the Flask API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling\n",
    "\n",
    "In order to use the model in the Flask app, it can be pickled. \n",
    "The pickle module, and the pickle file format, allows Python objects\n",
    "to be serialized and de-serialized. In this case, the trained vectorizer\n",
    "and model can be made into pickle files, which are then loaded into the\n",
    "Flask app for use in the recommendation API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create pickle func to make pickling (a little) easier === #\n",
    "\n",
    "def picklizer(to_pickle, filename, path):\n",
    "    \"\"\"\n",
    "    Creates a pickle file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    to_pickle : Python object\n",
    "        The trained / fitted instance of the \n",
    "        transformer or model to be pickled.\n",
    "    filename : string\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "    path : string or path-like object\n",
    "        The path to the desired output directory.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Create the path to save location\n",
    "    picklepath = os.path.join(path, filename)\n",
    "\n",
    "    # Use context manager to open file\n",
    "    with open(picklepath, \"wb\") as p:\n",
    "        pickle.dump(to_pickle, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Picklize! === #\n",
    "filepath = \"../../models\"\n",
    "\n",
    "# Export vectorizer as pickle\n",
    "picklizer(dtm, \"vec_01.pkl\", filepath)\n",
    "\n",
    "# Export knn model as pickle\n",
    "picklizer(nn, \"nn_01.pkl\", filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it again! (this time with pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the vdtm pickle into new object for testing\n",
    "vv_path = os.path.join(filepath, \"vec_01.pkl\")\n",
    "\n",
    "# Use context manager to open and load pickle file\n",
    "with open(vv_path, \"rb\") as p:\n",
    "    vv = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the knn pickle into new object for testing\n",
    "knn_path = os.path.join(filepath, \"nn_01.pkl\")\n",
    "\n",
    "# Use context manager to open and load pickle file\n",
    "with open(knn_path, \"rb\") as p:\n",
    "    nn2 = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Get (pick)les === #\n",
    "\n",
    "# Another slightly modified version that uses pickle objects\n",
    "\n",
    "def rec_pickle(req, n=10):\n",
    "    \"\"\"Function to recommend n subreddits given a request.\"\"\"\n",
    "    # Create vector from request\n",
    "    req_vec = vv.transform([req])\n",
    "\n",
    "    # Access the top n indexes\n",
    "    rec_id = nn2.kneighbors(req_vec.todense(), n_neighbors=n)[1][0]\n",
    "\n",
    "    # Index-locate the neighbors in original dataframe\n",
    "    rec_array = df1.iloc[rec_id][\"subreddit\"]\n",
    "\n",
    "    return rec_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106538      callofcthulhu\n",
       "571435               SCCM\n",
       "819094      adventuretime\n",
       "328843            TheWire\n",
       "482379        Cloververse\n",
       "657019             Ripple\n",
       "195943       bladeandsoul\n",
       "509055          ethtrader\n",
       "169129         instantpot\n",
       "906320       bladeandsoul\n",
       "805929            Hyundai\n",
       "991606          fragrance\n",
       "558214         Journalism\n",
       "14804            brandnew\n",
       "333284    DBZDokkanBattle\n",
       "767414             kindle\n",
       "104371         Machinists\n",
       "790023          reloading\n",
       "55396        digitalnomad\n",
       "253547         oculusnsfw\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Test out the modified function / pickle objects === #\n",
    "\n",
    "# This one comes from r/buildmeapc\n",
    "post3 = \"\"\"I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldn’t start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldn’t be more excited!\"\"\"\n",
    "\n",
    "# This time I'll pass in 20, because why not?\n",
    "post_3_recs = rec_pickle(post3, 20)\n",
    "post_3_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled Results\n",
    "\n",
    "Results of the pickled model are adequately terrible. But hey, it gives an output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
